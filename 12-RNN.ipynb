{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b895eeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, math, random\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "with open(\"/Users/mac/Desktop/code/tasksv11/en/qa1_single-supporting-fact_train.txt\", mode=\"r\") as f:\n",
    "    raw = f.readlines()\n",
    "\n",
    "tokens = []\n",
    "for line in raw[0:1000]:\n",
    "    \n",
    "    line = line.replace('\\t', '').replace('\\n', '').replace('?', '').replace('.', '').replace('1','').\\\n",
    "    replace('2','').replace('3','').replace('4','').replace('5','').replace('6','').replace('7','').\\\n",
    "    replace('8','').replace('9','').replace('0', '')\n",
    "    \n",
    "    tokens.append(line.lower().split(\" \")[1:])\n",
    "\n",
    "vocab = set()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        vocab.add(word)\n",
    "vocab = list(vocab)\n",
    "#print(len(vocab)) -19\n",
    "\n",
    "word2index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word2index[word] = index\n",
    "\n",
    "def words2indices(sentence):\n",
    "    idx = []\n",
    "    for word in sentence:\n",
    "        idx.append(word2index[word])\n",
    "    return idx\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x/e_x.sum(axis=0)\n",
    "    \n",
    "np.random.seed(1)\n",
    "embed_size = 10\n",
    "\n",
    "embed = (np.random.rand(len(vocab), embed_size) * 0.5) * 0.1\n",
    "reccurent = np.eye(embed_size)\n",
    "\n",
    "start = np.zeros(embed_size)\n",
    "\n",
    "decoder = (np.random.rand(embed_size, len(vocab)) * 0.5) * 0.1     #decoder.shape (10,19)\n",
    "\n",
    "one_hot = np.eye(len(vocab))\n",
    "\n",
    "def predict(sent):\n",
    "    \n",
    "    layers = list()\n",
    "    layer = {}\n",
    "    layer['hidden'] = start\n",
    "    layers.append(layer)\n",
    "    \n",
    "    loss = 0\n",
    "    \n",
    "    for target_i in range(len(sent)):\n",
    "        layer = {}\n",
    "        \n",
    "        #sent[target_i] -> red\n",
    "        #layers[-1]['hidden'] -> вектор start\n",
    "        layer['pred'] = softmax(layers[-1]['hidden'].dot(decoder))  # 1-10 x 10-19\n",
    "        \n",
    "        #layer['pred'].shape -> (19,)  прогноз вектора предложения\n",
    "        \n",
    "        loss += -np.log(layer['pred'][sent[target_i]])\n",
    "        #print(layer['pred'][sent[target_i]])\n",
    "        layer['hidden'] = layers[-1]['hidden'].dot(reccurent) + embed[sent[target_i]]\n",
    "        \n",
    "        layers.append(layer)\n",
    "        \n",
    "    return layers, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1ad9fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity:18.984618385977953\n",
      "Perplexity:18.96275005587188\n",
      "Perplexity:18.93877844404012\n",
      "Perplexity:18.91042845970859\n",
      "Perplexity:18.87457507827111\n",
      "Perplexity:18.82642452292392\n",
      "Perplexity:18.757899657589327\n",
      "Perplexity:18.654040755636426\n",
      "Perplexity:18.483848071440892\n",
      "Perplexity:18.17277888208176\n",
      "Perplexity:17.500489853797884\n",
      "Perplexity:15.688562123214313\n",
      "Perplexity:13.14588513029836\n",
      "Perplexity:12.140259499740603\n",
      "Perplexity:11.62391098031961\n",
      "Perplexity:11.578062588141536\n",
      "Perplexity:11.594517626270905\n",
      "Perplexity:11.57212820079712\n",
      "Perplexity:11.487371972762437\n",
      "Perplexity:11.293462213728668\n",
      "Perplexity:10.910145025194483\n",
      "Perplexity:10.207273551828141\n",
      "Perplexity:9.098117424679439\n",
      "Perplexity:7.841592694590467\n",
      "Perplexity:6.849939283163692\n",
      "Perplexity:6.258363207029411\n",
      "Perplexity:5.923915649024729\n",
      "Perplexity:5.7213340295686255\n",
      "Perplexity:5.582331292706548\n",
      "Perplexity:5.471672677975867\n",
      "Perplexity:5.376629772812686\n",
      "Perplexity:5.294176289883838\n",
      "Perplexity:5.222267616448059\n",
      "Perplexity:5.157470206818019\n",
      "Perplexity:5.095225090995714\n",
      "Perplexity:5.030596707770631\n",
      "Perplexity:4.959580290310304\n",
      "Perplexity:4.880693472562948\n",
      "Perplexity:4.796033646837253\n",
      "Perplexity:4.721363841292467\n"
     ]
    }
   ],
   "source": [
    "for iter in range(40000):\n",
    "    alpha = 0.001\n",
    "    sent = words2indices(tokens[iter%len(tokens)][1:])\n",
    "    #sent = words2indices(tokens[5][1:])\n",
    "    layers, loss = predict(sent)\n",
    "    #print(len(vocab))\n",
    "    #print(tokens[iter%len(tokens)][1:],tokens[iter%len(tokens)][:])\n",
    "    \n",
    "    for layer_idx in reversed(range(len(layers))):\n",
    "        layer = layers[layer_idx]\n",
    "        target = sent[layer_idx-1]\n",
    "        #print('layer_idx', layer_idx)\n",
    "        #print('layer:', layer)\n",
    "        #print('target', target)\n",
    "        #print('onehot', one_hot[target])\n",
    "        \n",
    "        #print(one_hot[2]) -> [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
    "        \n",
    "        if (layer_idx > 0):\n",
    "            layer['output_delta'] = layer['pred'] - one_hot[target]\n",
    "            new_hidden_delta = layer['output_delta'].dot(decoder.transpose())\n",
    "        \n",
    "            \n",
    "            if (layer_idx == len(layers) - 1):\n",
    "                layer['hidden_delta'] = new_hidden_delta\n",
    "            else:\n",
    "                layer['hidden_delta'] = new_hidden_delta + layers[layer_idx+1]['hidden_delta'].dot(reccurent.transpose())\n",
    "        \n",
    "        else:\n",
    "            layer['hidden_delta'] = layers[layer_idx+1]['hidden_delta'].dot(reccurent.transpose())\n",
    "                      \n",
    "    start -= layers[0]['hidden_delta'] * alpha / float(len(sent))\n",
    "    for layer_idx, layer in enumerate(layers[1:]):\n",
    "        \n",
    "        decoder -= np.outer(layers[layer_idx]['hidden'], layer['output_delta']) * alpha / float(len(sent))\n",
    "        \n",
    "        reccurent -= np.outer(layers[layer_idx]['hidden'], layer['hidden_delta']) * alpha / float(len(sent))\n",
    "        \n",
    "    if (iter%1000 == 0):\n",
    "        print(\"Perplexity:\" + str(np.exp(loss/len(sent))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdffe1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x_):\n",
    "    x = np.atleast_2d(x_)\n",
    "    temp = np.exp(x)\n",
    "    return temp/np.sum(temp, axis=1, keepdims=True)\n",
    "\n",
    "# word_vects = {}\n",
    "# word_vects['yankees'] = np.array([[0,0,0]])\n",
    "# word_vects['bears'] = np.array([[0,0,0]])\n",
    "# word_vects['braves'] = np.array([[0,0,0]])\n",
    "# word_vects['red'] = np.array([[0,0,0]])\n",
    "# word_vects['sox'] = np.array([[0,0,0]])\n",
    "# word_vects['lose'] = np.array([[0,0,0]])\n",
    "# word_vects['defeat'] = np.array([[0,0,0]])\n",
    "# word_vects['beat'] = np.array([[0,0,0]])\n",
    "# word_vects['tie'] = np.array([[0,0,0]])\n",
    "\n",
    "# sent2output = np.random.rand(3, len(word_vects))\n",
    "# identity = np.eye(3)\n",
    "\n",
    "# layer_0 = word_vects['red']\n",
    "# layer_1 = layer_0.dot(identity) + word_vects['sox']\n",
    "# layer_2 = layer_1.dot(identity) + word_vects['defeat']\n",
    "\n",
    "# pred = softmax(layer_2.dot(sent2output))\n",
    "\n",
    "# y = np.array([1,0,0,0,0,0,0,0,0])\n",
    "\n",
    "# pred_delta = pred - y\n",
    "# layer_2_delta = pred_delta.dot(sent2output.T)\n",
    "# defeat_delta = layer_2_delta * 1\n",
    "# layer_1_delta = layer_2_delta.dot(identity.T)\n",
    "# sox_delta = layer_1_delta * 1\n",
    "# layer_0_delta = layer_1_delta.dot(identity.T)\n",
    "# alpha = 0.01\n",
    "# word_vects['red'] -= layer_0_delta * alpha\n",
    "# word_vects['sox'] -= sox_delta * alpha\n",
    "# word_vects['defeat'] -= defeat_delta * alpha\n",
    "\n",
    "# identity -= np.outer(layer_0, layer_1_delta) * alpha\n",
    "# identity -= np.outer(layer_1, layer_2_delta) * alpha\n",
    "# sent2output -= np.outer(layer_2, pred_delta) * alpha\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
